\documentclass{beamer}
\usetheme[pageofpages=of,% String used between the current page and the
                         % total page count.
          bullet=circle,% Use circles instead of squares for bullets.
          titleline=true,% Show a line below the frame title.
          alternativetitlepage=true,% Use the fancy title page.
       %   titlepagelogo=logo-polito,% Logo for the first page.
       %   watermark=watermark-polito,% Watermark used in every page.
       %   watermarkheight=100px,% Height of the watermark.
       %   watermarkheightmult=4,% The watermark image is 4 times bigger
                                % than watermarkheight.
          ]{Torino}

\setbeamertemplate{footline}{
  \begin{beamercolorbox}[wd=\paperwidth,ht=1ex,dp=1ex]{footline}
    \vspace{5pt} \hspace{1em} \insertframenumber/\inserttotalframenumber
  \end{beamercolorbox}
}

\author{Brendon J. Brewer}
\title{STATS 331 -- Introduction to Bayesian Statistics}
\institute{The University of Auckland}
\date{}


\linespread{1.3}
\usepackage{minted}
\usepackage[utf8]{inputenc}
\usepackage{dsfont}
\newcommand{\given}{\,|\,}
\newcommand{\Var}{\textnormal{Var}}


\begin{document}

\frame{\titlepage}

\begin{frame}
\begin{center}
\includegraphics[width=0.3\textwidth]{images/drinking_game.png}
\end{center}

Do not attempt at home.

\end{frame}

\begin{frame}
\frametitle{Maths, Probability, and R}

\begin{itemize}
\item To understand and use Bayesian statistics, we will
require some mathematics and some R programming
skills.\pause
\item  In this lecture we will briefly review some of the concepts
that we will need.\pause
\item If you are a bit rusty, there will be plenty of opportunity to
brush up. If you are already a pro, great!
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Probability}
We will use probability extensively in this course. You may have previous
experience with it (e.g. from STATS 125), which may help. However, the way we
use it is different.



\end{frame}


\begin{frame}
\frametitle{Probability --- Product Rule --- Traditional}
If we have two {\color{red} events} $A$ and $B$, the probability that both
{\color{red} occur} is given by the product rule:

\begin{align}
P(A \cap B) &= P(A)P(B \given A)
\end{align}

The `$\cap$' means intersection, but we will not see it again.



\end{frame}

\begin{frame}
\frametitle{Probability --- Product Rule --- Bayesian}
If we have two {\color{blue} propositions} or
{\color{blue} statements}
 $A$ and $B$, the probability that both {\color{blue} are true}
is given by the product rule:

\begin{align}
P(A, B) &= P(A)P(B \given A) \\
        &= P(B)P(A \given B).
\end{align}\pause

Note the change in terminology, and the notation. The comma means `and',
and the vertical bar means `given'.



\end{frame}


\begin{frame}
\frametitle{Probability --- Bayesian}
\begin{itemize}
\item In 331 we apply probability to {\color{blue} propositions
or statements}, not events. \pause
\item Probabilities represent the fact that we don't know everything.
You can think of it as `plausibility'. \pause
\item There is not a concept of `randomness' in the
sense of variability.\pause
\item If a quantity varies (e.g., the maximum daily temperature $T$),
really thereâ€™s more than one quantity
$(T_{\rm yesterday}$, $T_{\rm today}, T_{\rm tomorrow}, ...)$. We may or may
not know their values.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{The Most General Product Rule}
The product rule also holds if every term has `given $C$' on the right hand
side, where $C$ is any third statement:

\begin{align}
P(A, B \given C) &= P(A \given C)P(B \given A, C).
\end{align}\pause

If this is a bit much at this point, don't worry --- we have ways of making it
seem easy.

\end{frame}


\begin{frame}
\frametitle{Product Rule Example}
Consider a particular individual person, who you know very little about.

\begin{itemize}
\item Suppose the probability the person is male is 50\%.
\item Suppose the probability that a male is taller than 6 feet is
30\%.
\item What is the probability that the person is both male and
taller than 6 feet?
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Product Rule Example}
By the product rule,

\begin{align}
P(\textnormal{male}, \textnormal{tall})
    &= P(\textnormal{male})P(\textnormal{tall} \given \textnormal{male})\\
    &= 0.5 \times 0.3\\
    &= 0.15.
\end{align}

\end{frame}

\begin{frame}
\frametitle{Product Rule --- Probability Interpretation}
Notice that the story was about a {\em single individual} whose properties
were {\em unknown to you}. This is Bayesian.\\[0.5em]\pause

A similar question could ask for the
{\em proportion of the population} that is male and tall.
The mathematics would be exactly the same, but the meaning of the quantities
is different.
\end{frame}

\begin{frame}
\frametitle{Probability --- Sum Rule}
If we have two propositions or
statements
 $A$ and $B$, the probability that either $A$ or $B$ is true (or both)
is given by the sum rule:
\begin{align}
P(A \vee B) &= P(A) + P(B) - P(A, B)
\end{align}\pause

The $\vee$ means `or'. To remember: if a question involves
`and', use the product rule. If it involves `or', use the sum rule.

\end{frame}

\begin{frame}
\frametitle{Probability --- Sum Rule --- Special Case}
We often deal with {\bf mutually exclusive} statements or propositions ---
both can't be true. E.g., `the balance in my bank account is $\$100.00$'
and `the balance in my bank account is $\$105.00$'.
In this case, the final term in the sum rule is equal to zero:
\begin{align}
P(A \vee B) &= P(A) + P(B)
\end{align}\pause

We use this frequently.


\end{frame}


\begin{frame}
\frametitle{The Most General Sum Rule}
We can write down the sum rule in its most general form by inserting
`given $C$' in the right hand side of every term, where $C$ is some third
proposition:

\begin{align}
P(A \vee B \given C) &= P(A \given C) + P(B \given C) - P(A, B \given C).
\end{align}

\end{frame}

\begin{frame}
\frametitle{Partition Theorem and Bayes' Theorem}
There are some other results, called the Partition Theorem
(you may have met this
in STATS 210) and Bayes' theorem. They are derived from the sum and product
rules, and we will meet them soon enough.\\[0.5em]\pause

As you can probably guess, we will use Bayes' theorem a lot.

\end{frame}



\begin{frame}
\frametitle{Random Variables}
We will need to use and understand `random variables'. This is awkward
terminology for a Bayesian, so we won't often say this. But basically
we are talking about {\em quantities that have probability distributions}.

\end{frame}


\begin{frame}
\frametitle{Discrete Probability Distributions: Example}

\begin{center}
\includegraphics[width=0.5\textwidth]{images/geometric.pdf}
\end{center}

We can use the sum rule to calculate things like $P(x > 3)$ which would be
the sum of a subset of the probabilities.

\end{frame}


\begin{frame}
\frametitle{Continuous Probability Distributions: Example}

\begin{center}
\includegraphics[width=0.5\textwidth]{images/normal.pdf}
\end{center}

We can still calculate things like $P(x > 3)$ which would be
given by a definite integral of the probability density function.

\end{frame}

\begin{frame}
\frametitle{Maths: The News}
\begin{itemize}
\item The {\bf bad news}: integration happens {\em all the time} in Bayesian
statistics --- mostly because of the sum rule. \\[0.5em]\pause
\item The {\bf good news}: I don't expect you to know how to do different
kinds of integrals. In the second half, we will have computational tools that
do them for us.\\[0.5em]\pause
\item I do expect you to know what integrals are and what integral you
{\em would} have to do to obtain a certain result such as $P(x > 3)$, but you
won't have to actually do the integral.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Continuous Probability Distributions: Example}

\begin{center}
\includegraphics[width=0.5\textwidth]{images/integral.pdf}
\end{center}
We could calculate $P(x \in [1, 2])$ using $\int_1^2 p(x) \, dx$, which is
the shaded pink area --- but we will have easier ways to do this.


\end{frame}


\begin{frame}
\frametitle{Integration $\leftrightarrow$ Summation Duality}
In most cases, if we have an equation that involves a summation symbol
for the discrete case ($\sum$), the equivalent equation for the continuous
case involves an integral ($\int$).
Informally, we can say
\begin{align}
\sum \equiv \int
\end{align}\pause
When writing general things that apply to both, we can choose either the
summation symbol or the integration symbol, and once we have a real context,
we will know which one it is.

\end{frame}


\begin{frame}
\frametitle{Example Involving Symmetry}
Suppose $x \sim \textnormal{Normal}(0, 1)$, so the probability density function
(PDF) is
\begin{align}
p(x) &= \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}x^2\right).
\end{align}
Find the value of the following integral:
\begin{align}
P(x > 0) &= \int_0^\infty p(x) \, dx = ?
\end{align}\pause
Since the integrand is symmetric (an `even function'), we know that the
probability must be 1/2.

\end{frame}


\begin{frame}
\frametitle{Example Involving a Discrete Distribution}
Suppose $y$ can be either 1, 2, 3, or 4, with probabilities
0.1, 0.1, 0.3, and 0.5 respectively.\pause

What is the probability that $y$ is either 2 or 3?\\\pause
{\bf Answer}:
\begin{align}
P(y = 2 \vee y=3) &= P(y=2) + P(y=3) \\
                  &= 0.1 + 0.3 \\
                  &= 0.4.
\end{align}

\end{frame}


\begin{frame}
\frametitle{Expected Value}
The expected value (or expectation) is a measure of where the
`centre' of a probability distribution is. It is a weighted average
of the possible values, with the probabilities (or density) providing
the weights. The definition for the discrete and continuous case:
\begin{align}
E(X) &= \sum x p(x) \\
E(X) &= \int x p(x) \, dx
\end{align}\pause
{\bf Note}: In this course, integrals written without limits are
{\em definite integrals over the whole (implicit) range}.

\end{frame}


\begin{frame}
\frametitle{Variance and Standard Deviation}
\begin{itemize}
\item Variance and standard deviation are measures of how wide
a probability distribution is.\pause
\item Standard deviation is usually more intuitive
when you are going to give it a number, while theoretical
results are usually `nicer' in terms of variance.\pause
\end{itemize}
\begin{align}
\Var(x) &= E\left[(X - E(X))^2\right] \\
\textnormal{sd}(x) &= \sqrt{\Var(x)}.
\end{align}

\end{frame}


\begin{frame}
\frametitle{Example: Two Distributions}

    \begin{columns} % Create two columns
        \column{0.5\textwidth} % Left column (50% width)
        Which of these two distributions has the higher expected value?
        1, 2, or are they the same?

        \column{0.5\textwidth} % Right column (50% width)
        \includegraphics[width=\linewidth]{images/two_distributions.pdf}

     \end{columns}

\end{frame}

\begin{frame}
\frametitle{Example: Two Distributions}

    \begin{columns} % Create two columns
        \column{0.5\textwidth} % Left column (50% width)
        Which of these two distributions has the higher
        standard deviation?
        1, 2, or are they the same?

        \column{0.5\textwidth} % Right column (50% width)
        \includegraphics[width=\linewidth]{images/two_distributions.pdf}

     \end{columns}

\end{frame}

\begin{frame}
\frametitle{Log and Exp}
We will occasionally make use of logarithms and exponentials.
Be familiar with them.\pause
\begin{align}
e^a e^b &= e^{a+b} \\
(e^a)^b &= e^{ab} \\
\log(ab) &= \log(a) + \log(b) \\
\log(a^b) &= b \log(a)
\end{align}\pause
Note: $e \approx 2.71828$ and if I write log without a base, I usually mean
natural log ($\log_e$ or $\ln$).
\end{frame}

\begin{frame}
\frametitle{R}

\begin{center}
\includegraphics[width=0.7\textwidth]{images/Rlogo.png}
\end{center}


\end{frame}


\begin{frame}[fragile]
\frametitle{Basics of R}
We will use R a fair bit. Here is a reminder of some basic
things, such as creating and using variables. The \mintinline{r}{>}
here is the R console prompt, not part of the code.
\begin{minted}{r}
> x = 5
> y = 3.2
> health = x + 3*y
> health
> [1] 14.6
\end{minted}

\end{frame}

\begin{frame}[fragile]
\frametitle{R Vectors}
We will use R vectors frequently. These are collections of values of the
same type (usually numeric/floating point values for us).
One way of creating vectors is with the \mintinline{r}{c()} function.
\begin{minted}{r}
> x = c(5, 3, -3.3)
> x
[1] 5.0 3.0 -3.3
\end{minted}
\end{frame}

\begin{frame}[fragile]
\frametitle{R Vectors}
Subsets of vectors can be accessed using the square brackets
\mintinline{r}{[]}.
\begin{minted}{r}
> y = c(5, 3, -3.3, 7.2)
> y[3]
[1] -3.3
> y[1:2]
[1] 5 3
> y > 2
[1]  TRUE  TRUE FALSE  TRUE
> y[y > 2]
[1] 5.0 3.0 7.2
\end{minted}
\end{frame}

\begin{frame}[fragile]
\frametitle{Some Functions for R Vectors}
\begin{minted}{r}
> a_sequence = seq(1, 2, by=0.2)
> a_sequence
[1] 1.0 1.2 1.4 1.6 1.8 2.0
> boring = rep(2, 4)
> boring
[1] 2 2 2 2
> sum(boring)
[1] 8
> length(boring)
[1] 4
\end{minted}
\end{frame}


\begin{frame}[fragile]
\frametitle{Probability Distributions in R}
Many probability distributions are built into R, and there
are functions for generating random numbers and for evaluating
the PDF or PMF. Example using Uniform$(0, 1)$:
\begin{minted}{r}
> runif(3)
[1] 0.7289419 0.9163833 0.5571766
> dunif(c(0.3, 0.7, 1.5))
[1] 1 1 0
\end{minted}
\end{frame}


\begin{frame}[fragile]
\frametitle{Standard Normal Distribution in R}
\begin{minted}{r}
> rnorm(3)
[1] -0.2732889  0.0432207 -0.3625170
> dnorm(c(1, -5, 10))
[1] 2.419707e-01 1.486720e-06 7.694599e-23
\end{minted}
\end{frame}


\begin{frame}[fragile]
\frametitle{Plotting Probability Distributions}
We will often have a vector of possible values of some
quantity (\mintinline{r}{x}, say) and a corresponding vector of
probabilities or density values
\mintinline{r}{p}. We can plot the distribution:
\begin{minted}{r}
x = seq(-10, 10, length=101)
p = dnorm(x, mean=3, sd=1)
plot(x, p, type="l")
\end{minted}
Note the use of optional arguments to functions.
\end{frame}


\begin{frame}[fragile]
\frametitle{R plot}

\begin{center}
\includegraphics[width=0.5\textwidth]{images/R_normal.pdf}
\end{center}
\vspace{-1.5em}
{\bf Note}: \mintinline{r}{type="h"} is often better for
discrete distributions.
\end{frame}

\begin{frame}[fragile]
\frametitle{Uses of dnorm etc.}
\begin{itemize}
\item In the previous example, \mintinline{r}{x} was a vector
and the mean and sd of the distribution were scalars.\pause
\item However we will often use \mintinline{r}{dnorm()}
for the {\em likelihood function} in which case it will be the
other way around.\pause
\item Sometimes we will need the {\em log} of the probability
or probability density. By passing \mintinline{r}{log=TRUE}
as another argument, we can get this.
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Help on Built-in R Functions}
\begin{itemize}
\item For help on the function \mintinline{r}{rnorm()}, do this:
\begin{minted}{r}
> ?rnorm
\end{minted}
\item If you don't know what function you're looking for, search
with \mintinline{r}{??}. E.g.:
\begin{minted}{r}
> ??poisson
\end{minted}
The R help pages will have way more information than you actually need.
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Summary}
\begin{itemize}
\item Maths: integration (the concepts, not hard integrals, computers
will generally do these for us).\pause
\item Stats: Probability, probability distributions, expected value,
variance and standard deviation.\pause
\item Working with R vectors.\pause
\item Working with probability distributions in R, using the d and the r
functions.
\end{itemize}
\end{frame}

\end{document}

