\documentclass{beamer}
\usetheme[pageofpages=of,% String used between the current page and the
                         % total page count.
          bullet=circle,% Use circles instead of squares for bullets.
          titleline=true,% Show a line below the frame title.
          alternativetitlepage=true,% Use the fancy title page.
       %   titlepagelogo=logo-polito,% Logo for the first page.
       %   watermark=watermark-polito,% Watermark used in every page.
       %   watermarkheight=100px,% Height of the watermark.
       %   watermarkheightmult=4,% The watermark image is 4 times bigger
                                % than watermarkheight.
          ]{Torino}

\setbeamertemplate{footline}{
  \begin{beamercolorbox}[wd=\paperwidth,ht=1ex,dp=1ex]{footline}
    \vspace{5pt} \hspace{1em} \insertframenumber/\inserttotalframenumber
  \end{beamercolorbox}
}

\author{Brendon J. Brewer}
\title{STATS 331 -- Introduction to Bayesian Statistics}
\institute{The University of Auckland}
\date{}


\linespread{1.3}
\usepackage{minted}
\usepackage[utf8]{inputenc}
\usepackage{dsfont}
\newcommand{\given}{\,|\,}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bmu}{\boldsymbol{\mu}}


\begin{document}

\frame{\titlepage}

\begin{frame}
\centering
\Large
Sports Prediction

\begin{center}
\includegraphics[width=0.6\textwidth]{images/football.jpg}

Credit: MDM, Wikimedia Commons
\end{center}

\end{frame}


\begin{frame}
\frametitle{Motivation}

\begin{itemize}
\item This is a `fun'\footnote{Your mileage may vary.}
lecture where each year I try to predict the NRL
(Rugby League tournament based in Australia) grand final result using Bayesian
statistics. \pause
\item It is somewhat related to logistic regression.\pause
\item The model will start off relatively simple, but will eventually
become a hierarchical model.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{NRL}

\begin{itemize}
\item The NRL is the (Australian) National Rugby League tournament (it also
includes the New Zealand Warriors).\pause
\item There are 17 teams in the competition.\pause
\item The grand final will be played on Sunday, between the teams:
TBD.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{The Data}
To predict the grand final result, we will use the data for who won every match
so far this season. We will fit a model with parameters describing the ability
of each team, and then do prediction in the usual way, in JAGS.\\[0.5em]\pause

I asked ChatGPT to write a little script to download this year's match results
from the web, and it succeeded!
\end{frame}


\begin{frame}[fragile]
\frametitle{The Data}

\begin{minted}{r}
> data = read.csv("nrl_2025_results.csv")
> head(data)
  home_team away_team home_score away_score home_team_win
1         1         2         30          8             1
2         3         4         28         22             1
3         5         6         14         50             0
4         7         8          8         10             0
5         9        10         16         14             1
6        11        12         20         28             0
\end{minted}

\end{frame}


\begin{frame}[fragile]
\frametitle{The Data}
For simplicity, we will only make use of the columns
\mintinline{r}{home_team} (the identity of the home team),
\mintinline{r}{away_team} (the identity of the away team),
and \mintinline{r}{home_team_win} (whether or not the home team won the match).
So we are throwing away information about
the scores and just looking at the winner of each match.

\end{frame}


\begin{frame}[fragile]
\frametitle{Assumptions}
\begin{itemize}
\item Some teams are better than others. When a good team plays a bad team, there is a high probability that the good team wins.\pause
\item If two teams of equal ability play, the probabilities of each team winning
will be 50/50.
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Ability --- the Bradley-Terry Model}
We will say that each team has a numerical `ability' --- a positive number which
describes how good they are. If team 1 plays against team 2, the probability
that team 1 wins is given by
\begin{align}
P(\textnormal{Team 1 wins} \given a_1, a_2)
    &= \frac{a_1}{a_1 + a_2}.
\end{align}
For example, if $a_1 = 3.5$ and $a_2 = 1.5$ then this probability is 0.7.\pause

{\bf Note:} Only ratios of abilities matter.

\end{frame}

\begin{frame}[fragile]
\frametitle{Bayesian Analysis}
\begin{itemize}
\item The 17 team abilities are the unknown parameters. \pause
\item We will update from a prior distribution for the abilities to a posterior
distribution based on the data (who won each match).\pause
\item We will use the posterior distribution to predict future data (Sunday's
match).
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Model 1: Prior}
The abilities are all positive, and are probably not crazily different from
each other. We will use the following lognormal prior:

\begin{minted}{r}
for(i in 1:17)
{
    log_ability[i] ~ dnorm(0, 1)
    ability[i] <- exp(log_ability[i])
}
\end{minted}


\end{frame}


\begin{frame}[fragile]
\frametitle{Model 1: Likelihood}
The sampling distribution/likelihood code is based on the Bradley-Terry formula
and the Bernoulli distribution (equivalent to Binomial with one trial):

\begin{minted}{r}
for(i in 1:length(home_team_win))
{
    p[i] <- ability[home_team[i]]/
            (ability[home_team[i]] + ability[away_team[i]])
    home_team_win[i] ~ dbern(p[i])
}
\end{minted}

\end{frame}


\begin{frame}[fragile]
\frametitle{Model 1: Results}
Let's run the model and check one or two of the trace plots.

\end{frame}


\begin{frame}[fragile]
\frametitle{Model 1: Prediction}
We will now make the grand final prediction. We have posterior samples
for the abilities of the two teams involved. Conditional on these, the Bradley-Terry formula gives us the probability of victory for each team.\\[0.5em]\pause

In JAGS, we just need to `add one more
unobserved data point' to the model.
\end{frame}



\begin{frame}[fragile]
\frametitle{Model 1: Prediction}

Recall that when predicting new data, there are two sources of uncertainty:
\begin{enumerate}
\item [(1)]
The sampling distribution only gives us probabilities in the first place.\pause
\item [(2)] And we don't know the parameters! We only have the posterior distribution
for them.
\end{enumerate}
\end{frame}


\begin{frame}[fragile]
\frametitle{Model 1: Prediction}

% TODO: Change team IDs when I know who's in the grand final
\begin{minted}{r}
p_new <- ability[1]/(ability[1] + ability[2])
\end{minted}
\pause
\begin{itemize}
\item The posterior mean of the conditional probability \mintinline{r}{p_new}
is the predictive probability.\pause
\item We could add \mintinline{r}{home_team_win_new ~ dbern(p_new)} but it's
not necessary in this type of model (similar to logistic regression).
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Model 1: Prediction}
Let's run the model and get the posterior mean for \mintinline{r}{p_new}.


\end{frame}

\end{document}

