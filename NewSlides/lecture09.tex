\documentclass{beamer}
\usetheme[pageofpages=of,% String used between the current page and the
                         % total page count.
          bullet=circle,% Use circles instead of squares for bullets.
          titleline=true,% Show a line below the frame title.
          alternativetitlepage=true,% Use the fancy title page.
       %   titlepagelogo=logo-polito,% Logo for the first page.
       %   watermark=watermark-polito,% Watermark used in every page.
       %   watermarkheight=100px,% Height of the watermark.
       %   watermarkheightmult=4,% The watermark image is 4 times bigger
                                % than watermarkheight.
          ]{Torino}

\setbeamertemplate{footline}{
  \begin{beamercolorbox}[wd=\paperwidth,ht=1ex,dp=1ex]{footline}
    \vspace{5pt} \hspace{1em} \insertframenumber/\inserttotalframenumber
  \end{beamercolorbox}
}

\author{Brendon J. Brewer}
\title{STATS 331 -- Introduction to Bayesian Statistics}
\institute{The University of Auckland}
\date{}


\linespread{1.3}
\usepackage{minted}
\usepackage[utf8]{inputenc}
\usepackage{dsfont}
\newcommand{\given}{\,|\,}

\begin{document}

\frame{\titlepage}

\begin{frame}
\begin{center}
\includegraphics[width=0.4\textwidth]{images/tshirt.png} \\
Credit: lesswrong.com
\end{center}

\end{frame}

\begin{frame}
\begin{center}
\Large
Introduction to MCMC and the Metropolis Algorithm
\end{center}

\end{frame}

\begin{frame}
\frametitle{Philosphy}
\begin{itemize}
\item Markov Chain Monte Carlo (MCMC) algorithms are, strictly speaking, not
Bayesian.\pause
\item However, they are associated with Bayesian statistics because they happen
to be very useful in this area.\pause
\item They allow us to describe posterior distributions in more than one
dimension (more than one unknown parameter) quite easily.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{More Than One Parameter}
So far we have focused on single-parameter problems until we get used to the
Bayesian approach. But most real problems have more than one unknown parameter.
Imagine making a Bayes' Box:
\begin{center}
{\tiny
\begin{tabular}{|c|c|c|c|c|}
\hline
Parameter & Prior & Likelihood & Prior $\times$ Likelihood & Posterior \\
$(\theta_1, \theta_2)$  & $p(\theta_1, \theta_2)$ & $p(x \given \theta_1, \theta_2)$ & $p(\theta_1, \theta_2)p(x\given\theta_1, \theta_2)$ & $p(\theta_1, \theta_2\given x)$ \\
\hline
(0, 0) &  &  & & \\
(0, 0.1) &  &  & & \\
... & ... & ... & ... & ... \\
(0.1, 0) &  &  & & \\
(0.1, 0.1) & & & & \\
... & ... & ... & ... & ... \\
(1, 0.9) & & & & \\
(1, 1) & & & & \\
\hline
Total & 1 & & & 1 \\
\hline
\end{tabular}
} % tiny
\end{center}

\end{frame}

\begin{frame}
\frametitle{More Than One Parameter}
\begin{itemize}
\item Because each hypothesis is about the value of the {\em pair}
$(\theta_1, \theta_2)$, instead of say 100 grid points, we would need
100 $\times$~100. \pause
\item This is quite feasible, but annoying, because you would need 2D arrays
instead of vectors.\pause
\item However, in even higher dimensions (let's say 100) you would need
$10^{100}$ rows (or something like that) in your Bayes' Box.
That is not going to happen.
\end{itemize}


\end{frame}


\begin{frame}[fragile]
\frametitle{Grids vs. Samples}
\begin{itemize}
\item You can think of the grid-based approach, which we have used so far,
as assigning {\bf weights} to the $\theta$ values. The posterior probabilities
are the weights, which is why you do calculations like
\mintinline{r}{sum(theta*post)} to get the posterior mean.\pause
\item The purpose of MCMC is to generate {\bf samples} from the posterior
distribution. This is a collection of $\theta$ values that implicitly have
equal weight. To get the posterior mean you would just do \mintinline{r}{mean(theta)}.
This is called {\bf Monte Carlo}.
\end{itemize}


\end{frame}


\begin{frame}[fragile]
\frametitle{Grids vs. Samples}
\centering
\includegraphics[width=0.6\textwidth]{images/grid_vs_samples.pdf}

\end{frame}

\begin{frame}
\frametitle{Using Monte Carlo Samples}
\begin{itemize}
\item If we can generate random numbers from the
posterior, then the histogram will look just like the posterior.\pause
\item If we have enough random numbers, we can use the
sample mean which will be almost the same as the
actual posterior mean.\pause
\item Same with sd or any other summary.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Large Sample Size}

\begin{itemize}
\item More samples means the generated $\theta$ values more accurately
reflect the posterior.\pause
\item But the posterior still has the uncertainty built into it, as represented
by the scatter in the generated $\theta$ values.\pause
\item We will assume that we always have a `large enough'
sample from the posterior. It is possible to worry about extra
uncertainty from the Monte Carlo approximation itself, but we will ignore this.
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Marginalisation}
Marginalisation is the process of going from a higher dimensional probability
distribution to a lower dimensional one. Consider the following table,
with a parameter $\theta_1$ on the $x$-axis and $\theta_2$ on the $y$-axis.
\begin{align}
\begin{array}{c|cccc}
4 & 1/16 & 1/12 & 1/8 & 1/4 \\
3 & 1/16 & 1/12 & 1/8 & 0 \\
2 & 1/16 & 1/12 & 0   & 0 \\
1 & 1/16 & 0    & 0   & 0 \\
\hline
  & 1    & 2    & 3   & 4
\end{array}
\end{align}
\pause
If we only really care about $\theta_1$, we sum the columns
to get the {\bf marginal distribution}.


\end{frame}


\begin{frame}
\frametitle{Marginalisation}
The result of the marginalisation is the following probability distribution
for $\theta_1$ by itself:

\begin{center}
\begin{align}
\left\{\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}\right\}
\end{align}
\end{center}

\end{frame}


\begin{frame}
\frametitle{General Marginalisation}
If there are parameters $\theta_1, ..., \theta_N$ that we care about,
and ``nuisance parameters'' $\phi_1, ..., \phi_M$ that we don't, we can
{\bf integrate out} the nuisance parameters (this is similar to the column
sum we just did).
\pause
\begin{align}
p(\theta_1, ..., \theta_N \given x)
    &= \int p(\theta_1, ..., \theta_N, \phi_1, ..., \phi_M \given x)
            \, d\phi_1 ... d\phi_M.
\end{align}
\pause
One of the benefits of Monte Carlo is that we can avoid doing this integral.
\end{frame}



\begin{frame}
\frametitle{Marginalisation}
\centering
\includegraphics[width=0.6\textwidth]{images/marginalisation.png}

\end{frame}

\begin{frame}
\frametitle{Marginalisation}
Our samples will usually be in a 2D array format (or an R data frame),
where each row is one sample from the joint posterior and each column is
a particular parameter.
To get a marginal distribution we simply look at one column at a time.
This is a lot easier than multiple integration!

\end{frame}



\end{document}

